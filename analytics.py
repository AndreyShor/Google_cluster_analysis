# -*- coding: utf-8 -*-
"""3 Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A277B_RPzVDLhCgQiyNt33jZi4t_yVPq
"""

from google.colab import files

uploaded = files.upload()

import pandas as pd
import io

machine_events = pd.read_csv(io.BytesIO(uploaded['machine_events.csv']), header=None, names=[
    'timestamp', 'machine_id', 'event_type', 'platform_id', 'capacity_cpu', 'capacity_memory'])

job_events = pd.read_csv(io.BytesIO(uploaded['part-01_job_events.csv']), header=None, names=[
    'timestamp', 'missing_info', 'job_id', 'event_type', 'user_name',
    'scheduling_class', 'job_name', 'logical_job_name'])

task_events = pd.read_csv(io.BytesIO(uploaded['part-01_task_events.csv']), header=None, names=[
    'timestamp', 'missing_info', 'job_id', 'task_index', 'machine_id', 'event_type',
    'user_name', 'scheduling_class', 'priority', 'resource_request_cpu',
    'resource_request_ram', 'resource_request_disk', 'different_machine_constraint'])

task_usage = pd.read_csv(io.BytesIO(uploaded['data_with_labels_small.csv']))

import numpy as np
# Convert timestamps to numeric
machine_events['timestamp'] = pd.to_numeric(machine_events['timestamp'], errors='coerce')
job_events['timestamp'] = pd.to_numeric(job_events['timestamp'], errors='coerce')
task_events['timestamp'] = pd.to_numeric(task_events['timestamp'], errors='coerce')

# Drop rows where timestamp couldn't convert (if any)
machine_events.dropna(subset=['timestamp'], inplace=True)
job_events.dropna(subset=['timestamp'], inplace=True)
task_events.dropna(subset=['timestamp'], inplace=True)

# Convert to integer (optional, recommended)
machine_events['timestamp'] = machine_events['timestamp'].astype(np.int64)
job_events['timestamp'] = job_events['timestamp'].astype(np.int64)
task_events['timestamp'] = task_events['timestamp'].astype(np.int64)

#Import Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from scipy.stats import anderson, kstest

sns.set(style='whitegrid')

#3.1
# Example: Retry daily summary calculation
job_events['day'] = (job_events['timestamp'] / 1e6 // 86400).astype(int)

daily_summary = job_events.groupby('day').agg({
    'user_name': 'nunique',
    'job_id': 'nunique'
}).rename(columns={'user_name':'unique_users','job_id':'unique_jobs'})

print(daily_summary.head())

tasks_per_job = task_events.groupby(['job_id', 'priority'])['task_index'].nunique().reset_index()

avg_tasks_priority = tasks_per_job.groupby('priority')['task_index'].mean().reset_index(name='avg_tasks')

print(avg_tasks_priority.sort_values('priority'))
sns.barplot(data=avg_tasks_priority, x='priority', y='avg_tasks', palette='Blues')
plt.title("Average Tasks per Job by Priority")
plt.show()

total_failures = task_events[task_events['event_type'] == 3].shape[0]  # 3 is FAIL
print(f"Total task failures: {total_failures}")

#3.2
total_downtime_seconds = 0

for machine, events in machine_events.groupby('machine_id'):
    # Sort clearly by timestamp
    events_sorted = events.sort_values('timestamp')

    # Initialize clearly
    remove_time = None
    downtime = 0

    # Loop over sorted events to correctly pair REMOVE â†’ ADD
    for _, row in events_sorted.iterrows():
        if row['event_type'] == 1:  # REMOVE event
            remove_time = row['timestamp']
        elif row['event_type'] == 0 and remove_time is not None:  # ADD event following REMOVE
            downtime += (row['timestamp'] - remove_time)/1e6
            remove_time = None  # Reset after pairing

    total_downtime_seconds += downtime

# Final output clearly formatted
print(f"Total downtime (seconds): {total_downtime_seconds:.2f}")

#3.3
# Re-create task_times clearly with explicit event types
task_times = task_events.pivot_table(
    index=['job_id', 'task_index'],
    columns='event_type',
    values='timestamp'
).reset_index()

# Explicitly selecting SCHEDULE (1) and FINISH (4) event timestamps
task_times = task_times[['job_id', 'task_index', 1, 4]].dropna()

# Rename columns explicitly for clarity
task_times.columns = ['job_id', 'task_index', 'schedule', 'finish']

# Clearly ensure 'finish' > 'schedule'
task_times = task_times[task_times['finish'] > task_times['schedule']]

# Calculate duration correctly
task_times['duration'] = (task_times['finish'] - task_times['schedule']) / 1e6

# Compute average task duration per job clearly
avg_durations = task_times.groupby('job_id')['duration'].mean().reset_index(name='avg_duration')

# Merge back to identify long tasks
merged = task_times.merge(avg_durations, on='job_id')

# Tasks 50% longer than job average
long_tasks = merged[merged['duration'] > 1.5 * merged['avg_duration']]

# Clearly display final results
print(long_tasks.head())

#3.4
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import anderson, kstest, norm

# Dynamically select the most common event type
selected_event_type = job_events['event_type'].value_counts().idxmax()

# Get submissions data by user
submissions = job_events[job_events['event_type'] == selected_event_type].groupby('user_name').size()

if submissions.size > 1 and submissions.std() > 0:
    # Visualize PDF
    sns.histplot(submissions, kde=True, stat='density', color='skyblue')
    plt.title("Probability Distribution of User Job Submissions")
    plt.xlabel("Number of Submissions per User")
    plt.ylabel("Density")
    plt.grid(True)
    plt.show()

    # Anderson-Darling Test
    ad_result = anderson(submissions, dist='norm')
    print("Anderson-Darling Test Statistic:", ad_result.statistic)

    # Kolmogorov-Smirnov Test
    ks_stat, ks_pvalue = kstest(submissions, 'norm', args=(submissions.mean(), submissions.std()))
    print(f"KS Statistic: {ks_stat:.4f}, p-value: {ks_pvalue:.4f}")

    if ks_pvalue < 0.05:
        print("Data is NOT normally distributed.")
    else:
        print("Data MAY be normally distributed.")
else:
    print("Insufficient data or variance for statistical analysis.")

#3.5
# Corrected merge (fixing column name case issue)
cpu_data = task_events.merge(
    task_usage,
    left_on=['job_id', 'task_index'],
    right_on=['job_ID', 'task_index'],
    how='inner'
)

# Select relevant features explicitly
cpu_data_clean = cpu_data[['user_name', 'resource_request_cpu', 'mean_CPU_usage_rate']].copy()

# Convert explicitly to numeric
cpu_data_clean['resource_request_cpu'] = pd.to_numeric(cpu_data_clean['resource_request_cpu'], errors='coerce')
cpu_data_clean['mean_CPU_usage_rate'] = pd.to_numeric(cpu_data_clean['mean_CPU_usage_rate'], errors='coerce')

# Drop missing or invalid values explicitly
cpu_data_clean.dropna(subset=['resource_request_cpu', 'mean_CPU_usage_rate'], inplace=True)

# Filter valid range explicitly
cpu_data_clean = cpu_data_clean[(cpu_data_clean['resource_request_cpu'].between(0,1)) &
                                (cpu_data_clean['mean_CPU_usage_rate'].between(0,1))]

# Aggregate at user level explicitly
user_cpu_data = cpu_data_clean.groupby('user_name').agg({
    'resource_request_cpu':'mean',
    'mean_CPU_usage_rate':'mean'
}).reset_index()

# Standardize the data explicitly
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(user_cpu_data[['resource_request_cpu', 'mean_CPU_usage_rate']])

# Apply K-means clustering explicitly
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
user_cpu_data['cluster'] = kmeans.fit_predict(X_scaled)

# Plot clusters explicitly
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.scatterplot(data=user_cpu_data, x='resource_request_cpu', y='mean_CPU_usage_rate', hue='cluster', palette='deep', s=100)

# Cluster centers explicitly
centers = scaler.inverse_transform(kmeans.cluster_centers_)
plt.scatter(centers[:,0], centers[:,1], s=200, c='red', marker='X', label='Centroids')

plt.title('User Clusters: CPU Requested vs Actual CPU Utilization')
plt.xlabel('Average CPU Requested')
plt.ylabel('Average Actual CPU Usage')
plt.legend()
plt.grid(True)
plt.show()

# Cluster summary explicitly
print(user_cpu_data.groupby('cluster')[['resource_request_cpu', 'mean_CPU_usage_rate']].mean())